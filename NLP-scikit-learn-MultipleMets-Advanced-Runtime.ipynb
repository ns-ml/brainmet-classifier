{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from numpy.random import RandomState\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import linear_model, svm, tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "dataset = load_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data,\n",
    "                                                         dataset.target,\n",
    "                                                         test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a vectorizer/classifier pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a grid search to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1,1), (1,2), (1,3), (1,4)],\n",
    "             'tfidf__use_idf': (True, False),\n",
    "             'clf__alpha': (1e-2, 1e-3)}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = gs_clf.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: False\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "best_parameters, score, _ = max(gs_clf.grid_scores_, key = lambda x: x[1])\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predicted = gs_clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      0.38      0.55        21\n",
      "          2       0.63      1.00      0.77        22\n",
      "\n",
      "avg / total       0.81      0.70      0.66        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                        target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8 13]\n",
      " [ 0 22]]\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use advanced options (Masino et al.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Static parameters and setup\n",
    "import sys\n",
    "seed = 5824565\n",
    "def concatenate(d1,d2):\n",
    "    d = d1.copy()\n",
    "    d.update(d2)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('/Users/oarnaout/Dropbox/Stats/multiple-mets/')\n",
    "import sklearnextensions as sklx\n",
    "import printers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#custom preprocessor to keep some stop words\n",
    "english_stopwords = filter(lambda w: w not in ['no', 'not', 'under'],\n",
    "                           stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def text_preprocessor(report_text):\n",
    "    # 1. Remove all punctuation, whitespaces\n",
    "    letters_only = re.sub('[^a-zA-Z0-9]', ' ', report_text) \n",
    "    \n",
    "    # 2. All lower caps and split to words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 3. Convert stopwords list to set for efficiency\n",
    "    stops = set(english_stopwords)\n",
    "    \n",
    "    # 4. Remove stop words\n",
    "    meaningful_words = [w for w in words if w not in stops]\n",
    "    \n",
    "    # 5. Join words back together\n",
    "    return(' '.join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classifiers and parameters to consider\n",
    "feature_parameters  = {\n",
    "                'vect__binary':(False, True),\n",
    "               'vect__ngram_range': ((1,1),(1,2),(1,3)),\n",
    "               'vect__analyzer' : ('word', 'char_wb')}\n",
    "nb_feature_parameters  = {'vect__ngram_range': ((1,1),(1,2),(1,3)),\n",
    "               'vect__analyzer' : ('word', 'char_wb')}\n",
    "use_spare_array = True\n",
    "use_binary_features = True\n",
    "classifiers = ({\n",
    "    'logistic_regression':(linear_model.LogisticRegression(),\n",
    "                           use_spare_array,\n",
    "                           not use_binary_features,\n",
    "                           concatenate(feature_parameters, {'clf__C': [1/x for x in [0.01, 0.1, 0.3, 1.0, 3.0, 10.0]]})),\n",
    "    'svm_linear':(svm.LinearSVC(tol=1e-6),\n",
    "                  use_spare_array,\n",
    "                  not use_binary_features,\n",
    "                  concatenate(feature_parameters, {'clf__C': [1/x for x in [0.01, 0.1, 0.3, 1.0, 3.0, 10.0]]})),\n",
    "    'svm_gaussian':(svm.SVC(tol=1e-6, kernel='rbf'),\n",
    "                    use_spare_array,\n",
    "                    not use_binary_features,\n",
    "                    concatenate(feature_parameters, {'clf__gamma': [.01, .03, 0.1],\n",
    "                                             'clf__C': [1/x for x in [0.01, 0.1, 0.3, 1.0, 3.0, 10.0]]})),\n",
    "    'decision_tree':(tree.DecisionTreeClassifier(criterion='entropy', random_state=RandomState(seed)),\n",
    "                     not use_spare_array,\n",
    "                     not use_binary_features,\n",
    "                     concatenate(feature_parameters,{'clf__max_depth': [2, 3, 4, 5, 6, 7 , 8, 9, 10, 15, 20]})),\n",
    "    'random_forest':(RandomForestClassifier(criterion='entropy', random_state=RandomState(seed)),\n",
    "                     not use_spare_array,\n",
    "                     not use_binary_features,\n",
    "                     concatenate(feature_parameters,{'clf__max_depth': [2, 3, 4, 5],\n",
    "                                                     'clf__n_estimators': [5, 25, 50, 100, 150, 200]})),\n",
    "    'naive_bayes':(BernoulliNB(alpha=1.0, binarize=None, fit_prior=True, class_prior=None),\n",
    "                   use_spare_array,\n",
    "                   use_binary_features,\n",
    "                   {'vect__ngram_range':((1,1),(1,2),(1,3)),\n",
    "                    'vect__analyzer':('word', 'char_wb')})\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_file = 'text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   53.5s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=2)]: Done 1080 out of 1080 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete in 13.912677 sec\n",
      "\n",
      "------------------------------ svm_gaussian Grid Search Results ------------------------------\n",
      "Best score: 0.778\n",
      "Best parameter set:\n",
      "\tclf: SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=1e-06, verbose=False)\n",
      "\tclf__C: 100.0\n",
      "\tclf__cache_size: 200\n",
      "\tclf__class_weight: None\n",
      "\tclf__coef0: 0.0\n",
      "\tclf__decision_function_shape: None\n",
      "\tclf__degree: 3\n",
      "\tclf__gamma: 0.01\n",
      "\tclf__kernel: 'rbf'\n",
      "\tclf__max_iter: -1\n",
      "\tclf__probability: False\n",
      "\tclf__random_state: None\n",
      "\tclf__shrinking: True\n",
      "\tclf__tol: 1e-06\n",
      "\tclf__verbose: False\n",
      "\tsteps: [('vect', CountVectorizer(analyzer='word', binary=True, decode_error='ignore',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1),\n",
      "        preprocessor=<function text_preprocessor at 0x10d609320>,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)), ('clf', SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=1e-06, verbose=False))]\n",
      "\tvect: CountVectorizer(analyzer='word', binary=True, decode_error='ignore',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1),\n",
      "        preprocessor=<function text_preprocessor at 0x10d609320>,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__binary: True\n",
      "\tvect__decode_error: 'ignore'\n",
      "\tvect__dtype: <type 'numpy.int64'>\n",
      "\tvect__encoding: u'utf-8'\n",
      "\tvect__input: 'content'\n",
      "\tvect__lowercase: True\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: None\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__preprocessor: <function text_preprocessor at 0x10d609320>\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: None\n",
      "\tvect__token_pattern: u'(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
      "\tvect__tokenizer: None\n",
      "\tvect__vocabulary: None\n",
      "\n",
      "Test Data Performance with Best Estimator Parameters\n",
      "\n",
      "Accuracy:\t\t\t0.7907\n",
      "F1-Score:\t\t\t0.8302\n",
      "PPV/Precision tp/pp:\t\t0.7097\n",
      "NPV tn/pn:\t\t\t1.0000\n",
      "Sensitivity/Recall tp/[tp+fn]:\t1.0000\n",
      "Specificity tn/[tn+fp]:\t\t0.5714\n",
      "\n",
      "\tConfusion Matrix\n",
      "A  \t   Predicted\n",
      "c  \t   Normal\tAbnormal\n",
      "t  Normal      12\t9\n",
      "u  Abnormal    0\t22\n",
      "a  \n",
      "l  \n",
      "Performing grid search...\n",
      "Fitting 5 folds for each of 132 candidates, totalling 660 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=2)]: Done 660 out of 660 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete in 8.551398 sec\n",
      "\n",
      "------------------------------ decision_tree Grid Search Results ------------------------------\n",
      "Best score: 0.725\n",
      "Best parameter set:\n",
      "\tclf: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False,\n",
      "            random_state=<mtrand.RandomState object at 0x10d634a90>,\n",
      "            splitter='best')\n",
      "\tclf__class_weight: None\n",
      "\tclf__criterion: 'entropy'\n",
      "\tclf__max_depth: 6\n",
      "\tclf__max_features: None\n",
      "\tclf__max_leaf_nodes: None\n",
      "\tclf__min_samples_leaf: 1\n",
      "\tclf__min_samples_split: 2\n",
      "\tclf__min_weight_fraction_leaf: 0.0\n",
      "\tclf__presort: False\n",
      "\tclf__random_state: <mtrand.RandomState object at 0x10d634a90>\n",
      "\tclf__splitter: 'best'\n",
      "\tsa: <sklearnextensions.SparseToArray instance at 0x10d6b10e0>\n",
      "\tsteps: [('vect', CountVectorizer(analyzer='word', binary=True, decode_error='ignore',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1),\n",
      "        preprocessor=<function text_preprocessor at 0x10d609320>,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)), ('sa', <sklearnextensions.SparseToArray instance at 0x10d6b10e0>), ('clf', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False,\n",
      "            random_state=<mtrand.RandomState object at 0x10d634a90>,\n",
      "            splitter='best'))]\n",
      "\tvect: CountVectorizer(analyzer='word', binary=True, decode_error='ignore',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1),\n",
      "        preprocessor=<function text_preprocessor at 0x10d609320>,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__binary: True\n",
      "\tvect__decode_error: 'ignore'\n",
      "\tvect__dtype: <type 'numpy.int64'>\n",
      "\tvect__encoding: u'utf-8'\n",
      "\tvect__input: 'content'\n",
      "\tvect__lowercase: True\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: None\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__preprocessor: <function text_preprocessor at 0x10d609320>\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: None\n",
      "\tvect__token_pattern: u'(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
      "\tvect__tokenizer: None\n",
      "\tvect__vocabulary: None\n",
      "\n",
      "Test Data Performance with Best Estimator Parameters\n",
      "\n",
      "Accuracy:\t\t\t0.7442\n",
      "F1-Score:\t\t\t0.7556\n",
      "PPV/Precision tp/pp:\t\t0.7391\n",
      "NPV tn/pn:\t\t\t0.7500\n",
      "Sensitivity/Recall tp/[tp+fn]:\t0.7727\n",
      "Specificity tn/[tn+fp]:\t\t0.7143\n",
      "\n",
      "\tConfusion Matrix\n",
      "A  \t   Predicted\n",
      "c  \t   Normal\tAbnormal\n",
      "t  Normal      15\t6\n",
      "u  Abnormal    5\t17\n",
      "a  \n",
      "l  \n",
      "Performing grid search...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   50.1s\n",
      "[Parallel(n_jobs=2)]: Done 360 out of 360 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete in 5.537658 sec\n",
      "\n",
      "------------------------------ logistic_regression Grid Search Results ------------------------------\n",
      "Best score: 0.830\n",
      "Best parameter set:\n",
      "\tclf: LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\tclf__C: 10.0\n",
      "\tclf__class_weight: None\n",
      "\tclf__dual: False\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__intercept_scaling: 1\n",
      "\tclf__max_iter: 100\n",
      "\tclf__multi_class: 'ovr'\n",
      "\tclf__n_jobs: 1\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__random_state: None\n",
      "\tclf__solver: 'liblinear'\n",
      "\tclf__tol: 0.0001\n",
      "\tclf__verbose: 0\n",
      "\tclf__warm_start: False\n",
      "\tsteps: [('vect', CountVectorizer(analyzer='char_wb', binary=True, decode_error='ignore',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3),\n",
      "        preprocessor=<function text_preprocessor at 0x10d609320>,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)), ('clf', LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))]\n",
      "\tvect: CountVectorizer(analyzer='char_wb', binary=True, decode_error='ignore',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3),\n",
      "        preprocessor=<function text_preprocessor at 0x10d609320>,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)\n",
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__binary: True\n",
      "\tvect__decode_error: 'ignore'\n",
      "\tvect__dtype: <type 'numpy.int64'>\n",
      "\tvect__encoding: u'utf-8'\n",
      "\tvect__input: 'content'\n",
      "\tvect__lowercase: True\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: None\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\tvect__preprocessor: <function text_preprocessor at 0x10d609320>\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: None\n",
      "\tvect__token_pattern: u'(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
      "\tvect__tokenizer: None\n",
      "\tvect__vocabulary: None\n",
      "\n",
      "Test Data Performance with Best Estimator Parameters\n",
      "\n",
      "Accuracy:\t\t\t0.7442\n",
      "F1-Score:\t\t\t0.7843\n",
      "PPV/Precision tp/pp:\t\t0.6897\n",
      "NPV tn/pn:\t\t\t0.8571\n",
      "Sensitivity/Recall tp/[tp+fn]:\t0.9091\n",
      "Specificity tn/[tn+fp]:\t\t0.5714\n",
      "\n",
      "\tConfusion Matrix\n",
      "A  \t   Predicted\n",
      "c  \t   Normal\tAbnormal\n",
      "t  Normal      12\t9\n",
      "u  Abnormal    2\t20\n",
      "a  \n",
      "l  \n",
      "Performing grid search...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  68 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=2)]: Done 218 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=2)]: Done 360 out of 360 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete in 5.205982 sec\n",
      "\n",
      "------------------------------ svm_linear Grid Search Results ------------------------------\n",
      "Best score: 0.819\n",
      "Best parameter set:\n",
      "\tclf: LinearSVC(C=100.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=1e-06,\n",
      "     verbose=0)\n",
      "\tclf__C: 100.0\n",
      "\tclf__class_weight: None\n",
      "\tclf__dual: True\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__intercept_scaling: 1\n",
      "\tclf__loss: 'squared_hinge'\n",
      "\tclf__max_iter: 1000\n",
      "\tclf__multi_class: 'ovr'\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__random_state: None\n",
      "\tclf__tol: 1e-06\n",
      "\tclf__verbose: 0\n",
      "\tsteps: [('vect', CountVectorizer(analyzer='char_wb', binary=True, decode_error='ignore',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3),\n",
      "        preprocessor=<function text_preprocessor at 0x10d609320>,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)), ('clf', LinearSVC(C=100.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=1e-06,\n",
      "     verbose=0))]\n",
      "\tvect: CountVectorizer(analyzer='char_wb', binary=True, decode_error='ignore',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3),\n",
      "        preprocessor=<function text_preprocessor at 0x10d609320>,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)\n",
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__binary: True\n",
      "\tvect__decode_error: 'ignore'\n",
      "\tvect__dtype: <type 'numpy.int64'>\n",
      "\tvect__encoding: u'utf-8'\n",
      "\tvect__input: 'content'\n",
      "\tvect__lowercase: True\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: None\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\tvect__preprocessor: <function text_preprocessor at 0x10d609320>\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: None\n",
      "\tvect__token_pattern: u'(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
      "\tvect__tokenizer: None\n",
      "\tvect__vocabulary: None\n",
      "\n",
      "Test Data Performance with Best Estimator Parameters\n",
      "\n",
      "Accuracy:\t\t\t0.7442\n",
      "F1-Score:\t\t\t0.7843\n",
      "PPV/Precision tp/pp:\t\t0.6897\n",
      "NPV tn/pn:\t\t\t0.8571\n",
      "Sensitivity/Recall tp/[tp+fn]:\t0.9091\n",
      "Specificity tn/[tn+fp]:\t\t0.5714\n",
      "\n",
      "\tConfusion Matrix\n",
      "A  \t   Predicted\n",
      "c  \t   Normal\tAbnormal\n",
      "t  Normal      12\t9\n",
      "u  Abnormal    2\t20\n",
      "a  \n",
      "l  \n",
      "Performing grid search...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  30 out of  30 | elapsed:    7.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete in 0.455788 sec\n",
      "\n",
      "------------------------------ naive_bayes Grid Search Results ------------------------------\n",
      "Best score: 0.719\n",
      "Best parameter set:\n",
      "\tclf: BernoulliNB(alpha=1.0, binarize=None, class_prior=None, fit_prior=True)\n",
      "\tclf__alpha: 1.0\n",
      "\tclf__binarize: None\n",
      "\tclf__class_prior: None\n",
      "\tclf__fit_prior: True\n",
      "\tsteps: [('vect', CountVectorizer(analyzer='word', binary=True, decode_error='ignore',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1),\n",
      "        preprocessor=<function text_preprocessor at 0x10d609320>,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)), ('clf', BernoulliNB(alpha=1.0, binarize=None, class_prior=None, fit_prior=True))]\n",
      "\tvect: CountVectorizer(analyzer='word', binary=True, decode_error='ignore',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1),\n",
      "        preprocessor=<function text_preprocessor at 0x10d609320>,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__binary: True\n",
      "\tvect__decode_error: 'ignore'\n",
      "\tvect__dtype: <type 'numpy.int64'>\n",
      "\tvect__encoding: u'utf-8'\n",
      "\tvect__input: 'content'\n",
      "\tvect__lowercase: True\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: None\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__preprocessor: <function text_preprocessor at 0x10d609320>\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: None\n",
      "\tvect__token_pattern: u'(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
      "\tvect__tokenizer: None\n",
      "\tvect__vocabulary: None\n",
      "\n",
      "Test Data Performance with Best Estimator Parameters\n",
      "\n",
      "Accuracy:\t\t\t0.7209\n",
      "F1-Score:\t\t\t0.7692\n",
      "PPV/Precision tp/pp:\t\t0.6667\n",
      "NPV tn/pn:\t\t\t0.8462\n",
      "Sensitivity/Recall tp/[tp+fn]:\t0.9091\n",
      "Specificity tn/[tn+fp]:\t\t0.5238\n",
      "\n",
      "\tConfusion Matrix\n",
      "A  \t   Predicted\n",
      "c  \t   Normal\tAbnormal\n",
      "t  Normal      11\t10\n",
      "u  Abnormal    2\t20\n",
      "a  \n",
      "l  \n",
      "Performing grid search...\n",
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   56.8s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=2)]: Done 1440 out of 1440 | elapsed:  8.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete in 21.049066 sec\n",
      "\n",
      "------------------------------ random_forest Grid Search Results ------------------------------\n",
      "Best score: 0.766\n",
      "Best parameter set:\n",
      "\tclf: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
      "            oob_score=False,\n",
      "            random_state=<mtrand.RandomState object at 0x10d5c0fd0>,\n",
      "            verbose=0, warm_start=False)\n",
      "\tclf__bootstrap: True\n",
      "\tclf__class_weight: None\n",
      "\tclf__criterion: 'entropy'\n",
      "\tclf__max_depth: 4\n",
      "\tclf__max_features: 'auto'\n",
      "\tclf__max_leaf_nodes: None\n",
      "\tclf__min_samples_leaf: 1\n",
      "\tclf__min_samples_split: 2\n",
      "\tclf__min_weight_fraction_leaf: 0.0\n",
      "\tclf__n_estimators: 50\n",
      "\tclf__n_jobs: 1\n",
      "\tclf__oob_score: False\n",
      "\tclf__random_state: <mtrand.RandomState object at 0x10d5c0fd0>\n",
      "\tclf__verbose: 0\n",
      "\tclf__warm_start: False\n",
      "\tsa: <sklearnextensions.SparseToArray instance at 0x10beb6998>\n",
      "\tsteps: [('vect', CountVectorizer(analyzer='char_wb', binary=False, decode_error='ignore',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3),\n",
      "        preprocessor=<function text_preprocessor at 0x10d609320>,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)), ('sa', <sklearnextensions.SparseToArray instance at 0x10beb6998>), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
      "            oob_score=False,\n",
      "            random_state=<mtrand.RandomState object at 0x10d5c0fd0>,\n",
      "            verbose=0, warm_start=False))]\n",
      "\tvect: CountVectorizer(analyzer='char_wb', binary=False, decode_error='ignore',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3),\n",
      "        preprocessor=<function text_preprocessor at 0x10d609320>,\n",
      "        stop_words=None, strip_accents=None,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)\n",
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__binary: False\n",
      "\tvect__decode_error: 'ignore'\n",
      "\tvect__dtype: <type 'numpy.int64'>\n",
      "\tvect__encoding: u'utf-8'\n",
      "\tvect__input: 'content'\n",
      "\tvect__lowercase: True\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: None\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\tvect__preprocessor: <function text_preprocessor at 0x10d609320>\n",
      "\tvect__stop_words: None\n",
      "\tvect__strip_accents: None\n",
      "\tvect__token_pattern: u'(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
      "\tvect__tokenizer: None\n",
      "\tvect__vocabulary: None\n",
      "\n",
      "Test Data Performance with Best Estimator Parameters\n",
      "\n",
      "Accuracy:\t\t\t0.7209\n",
      "F1-Score:\t\t\t0.7857\n",
      "PPV/Precision tp/pp:\t\t0.6471\n",
      "NPV tn/pn:\t\t\t1.0000\n",
      "Sensitivity/Recall tp/[tp+fn]:\t1.0000\n",
      "Specificity tn/[tn+fp]:\t\t0.4286\n",
      "\n",
      "\tConfusion Matrix\n",
      "A  \t   Predicted\n",
      "c  \t   Normal\tAbnormal\n",
      "t  Normal      9\t12\n",
      "u  Abnormal    0\t22\n",
      "a  \n",
      "l  \n"
     ]
    }
   ],
   "source": [
    "for key, value in classifiers.items():\n",
    "    clf = value[0] #classifier\n",
    "    usa = value[1] #use sparse array\n",
    "    ubf = value[2] #use binary (for NB)\n",
    "    parameters = value[3]\n",
    "    vectorizer = CountVectorizer(input='content', decode_error='ignore', preprocessor=text_preprocessor, binary=ubf)\n",
    "    pipeline = (Pipeline(steps=[('vect', vectorizer),('clf',clf)]) if usa\n",
    "                    else Pipeline(steps=[('vect', vectorizer),('sa',sklx.SparseToArray()),('clf',clf)]))\n",
    "    gs = sklx.grid_analysis(pipeline, parameters, docs_train, y_train)\n",
    "    printers.print_grid_search_results(gs,key,out_file, docs_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the best parameters for the winning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_parameters = gs.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "             max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "             oob_score=False,\n",
       "             random_state=<mtrand.RandomState object at 0x10d5c0fd0>,\n",
       "             verbose=0, warm_start=False),\n",
       " 'clf__bootstrap': True,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__criterion': 'entropy',\n",
       " 'clf__max_depth': 4,\n",
       " 'clf__max_features': 'auto',\n",
       " 'clf__max_leaf_nodes': None,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__n_estimators': 50,\n",
       " 'clf__n_jobs': 1,\n",
       " 'clf__oob_score': False,\n",
       " 'clf__random_state': <mtrand.RandomState at 0x10d5c0fd0>,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False,\n",
       " 'sa': <sklearnextensions.SparseToArray instance at 0x10beb6998>,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='char_wb', binary=False, decode_error='ignore',\n",
       "           dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 3),\n",
       "           preprocessor=<function text_preprocessor at 0x10d609320>,\n",
       "           stop_words=None, strip_accents=None,\n",
       "           token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
       "           vocabulary=None)),\n",
       "  ('sa', <sklearnextensions.SparseToArray instance at 0x10beb6998>),\n",
       "  ('clf',\n",
       "   RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "               max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "               oob_score=False,\n",
       "               random_state=<mtrand.RandomState object at 0x10d5c0fd0>,\n",
       "               verbose=0, warm_start=False))],\n",
       " 'vect': CountVectorizer(analyzer='char_wb', binary=False, decode_error='ignore',\n",
       "         dtype=<type 'numpy.int64'>, encoding=u'utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 3),\n",
       "         preprocessor=<function text_preprocessor at 0x10d609320>,\n",
       "         stop_words=None, strip_accents=None,\n",
       "         token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
       "         vocabulary=None),\n",
       " 'vect__analyzer': 'char_wb',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'ignore',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': u'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__preprocessor': <function __main__.text_preprocessor>,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__vocabulary': None}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = concatenate(feature_parameters,{'clf__max_depth': [2, 3, 4, 5],\n",
    "                                                     'clf__n_estimators': [5, 25, 50, 100, 150, 200]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__max_depth: 4\n",
      "\tclf__n_estimators: 50\n",
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__binary: False\n",
      "\tvect__ngram_range: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predict on runtime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rundata = pd.read_excel('Jun21_2016_OneSurgeryPreOpMRI.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rundata.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rundata['Predicted']=gs.predict(rundata['Report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rundata['Predicted']=rundata['Predicted'].replace(0, 'single').replace(1, 'multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rundata.to_excel('runtime_data_catagorized_OA7.11.16.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
